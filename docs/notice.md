---
layout: default
title: 버그 신고 및 정오표
description: 코드/도서 오류를 리포트하는 방법과 정정 결과를 안내합니다.
---



## 버그 리포트하기

꼼꼼히 확인한다고 했지만 책 내용이나 코드에 오류나 버그가 있을 수 있습니다. 이 경우 [이곳](https://github.com/ratsgo/embedding/issues/new)에 접속해 이슈(issue)를 작성하시면 됩니다.

이슈를 만드는 방법은 다음과 같습니다. 아래 그림의 양식을 채워 `Submit new issue` 버튼을 누르면 이슈가 완성됩니다.



<img src="https://i.imgur.com/9tVHsZ8.png" width="600px" title="source: imgur.com" /><



## 정정 결과

도서 및 코드의 주요 정정 결과를 안내합니다.

**P. 32~33**

```
구체적으로는 첫 번째 단어 벡터 + 두 번째 단어 벡터 - 세 번째 단어 벡터를 계산해 보는 것이다. 그림 1-3처럼 아들 + 딸 - 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다. (중략) 단어1 + 단어2 - 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어들이 네 번째 열의 단어들이다.
>
구체적으로는 첫 번째 단어 벡터 - 두 번째 단어 벡터 + 세 번째 단어 벡터를 계산해 보는 것이다. 그림 1-3처럼 아들 - 딸 + 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다. (중략) 단어1 - 단어2 + 단어3 연산을 수행한 벡터와 코사인 유사도가 가장 높은 단어들이 네 번째 열의 단어들이다.
```

**P. 40**

```
우리가 풀고 싶은 자연어 처리의 구체적 문제들을 다운스트림 태스크(downstearm task)라고 한다.
>
우리가 풀고 싶은 자연어 처리의 구체적 문제들을 다운스트림 태스크(downstream task)라고 한다.
```

**P. 72**

```
언어학자들이 제시하는 품사 분류 기준은 기능(function), 의미(meaning), 형태(form) 등 세 가지다. 
> 
학교문법에 따르면 품사 분류 기준은 기능(function), 의미(meaning), 형식(form) 등 세 가지다.
```

**P. 122**

```
수식 4-6에서 f(w_i)란 해당 단어가 말뭉치에서 차지하는 비율(해당 단어 빈도/어휘 집합 크기)을 의미한다.
>
수식 4-6에서 U(w_i)란 해당 단어의 유니 그램 확률(해당 단어 빈도/전체 단어 수)을 의미한다.
수식 4-6 : f(w_i)를 U(w_i)로 대체
```

**P. 125**

```
모델 학습이 완료되면 U만 d차원의 단어 임베딩으로 쓸 수도 있고, U+V 행렬을 임베딩으 로 쓸 수도 있다. 혹은 U, V를 이어 붙여 2d 차원의 단어 임베딩으로 사용할 수도 있다.
>
모델 학습이 완료되면 U만 d차원의 단어 임베딩으로 쓸 수도 있고, U+V^T 행렬을 임베딩으로 쓸 수도 있다. 혹은 U, V^T를 이어 붙여 2d 차원의 단어 임베딩으로 사용할 수도 있다.
```

**P. 123**

```
서브샘플링 확률은 수식 4-8과 같다. (중략) 만일 f(w_i)가 0.01로 나타나는 빈도 높은 단어(예컨대 조사 은/는)는 위 식으로 계산한 P(w_i)가 0.9684나 돼서 해당 단어가 가질 수 있는 100번의 학습 기회 가운데 3~4번 정도는 학습에서 제외하게 된다. 반대로 등장 비율이 적어 P(w_i)가 0에 가깝다면 해당 단어가 나올 때마다 빼놓지 않고 학습을 시키는 구조다.
>
서브샘플링 확률은 수식 4-8과 같다. f(w_i)는 w_i의 빈도를 가리키며 t는 하이퍼파라메터이다. Mikolov et al. (2013b)은 t를 10^{-5}로 설정했다. (중략) 만일 f(w_i)가 0.01로 나타나는 빈도 높은 단어(예컨대 조사 은/는)는 위 식으로 계산한 P_{subsampling}(w_i)가 0.9684나 돼서 해당 단어가 가질 수 있는 100번의 학습 기회 가운데 96번 정도는 학습에서 제외하게 된다. 반대로 등장 비율이 적어 P_{subsampling}(w_i)가 0에 가깝다면 해당 단어가 나올 때마다 빼놓지 않고 학습을 시키는 구조다.
 ```

**P. 145**

```
이밖에 U+V, U와 V를 이어 붙여 임베딩으로 사용하는 것도 가능하다.
>
이밖에 U+V^T, U와 V^T를 이어 붙여 임베딩으로 사용하는 것도 가능하다.
```

**P. 181**

```
그림 5-3 왼쪽의 리스트는 해당 마크다운 문서의 영문 제목명과 그에 해당하는 문서 임베딩의 코사인 유사도를 가리킨다. (중략) 그림 5-3의 우측 그림은 실제 블로그 에서 Related Posts로 제시하고 있는 문서 목록이다. 
>
그림 5-3 오른쪽의 리스트는 해당 마크다운 문서의 영문 제목명과 그에 해당하는 문서 임베딩의 코사인 유사도를 가리킨다. (중략) 그림 5-3의 좌측 그림은 실제 블로그 에서 Related Posts로 제시하고 있는 문서 목록이다. 
```

**P. 211**

그림 5-18을 다음으로 교체

<a href="https://imgur.com/lhrbOGB"><img src="https://i.imgur.com/lhrbOGB.png" width="700px" title="source: imgur.com" /></a>


**P. 213**

```
gamma^task는 해당 태스크가 얼마나 중요한지 뜻하는 가중치를 의미한다.
>
gamma^task는 전체 ELMo 벡터의 크기를 스케일해 태스크 수행을 돕는 역할을 한다.
```

**P. 222**

```
개별 소프트맥스 값이 지나치게 작아지는 것을 방지 
> 
소프트맥스의 그래디언트가 지나치게 작아지는 것을 방지
```

**P. 225~226**

```
Pointwise Feedforward Networks
>
Position-wise Feedforward Networks
```

**P. 230**

```
전체 학습 데이터 토큰의 15%를 마스킹한다 
> 
학습 데이터 한 문장 토큰의 15%를 마스킹한다
```